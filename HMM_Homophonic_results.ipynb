{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM for decryption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CipherUtils import CipherGenerator\n",
    "from src.CipherUtils import TextEncoder\n",
    "from src.ProbabilityMatrix import ProbabilityMatrix\n",
    "from src.CipherUtils import TextPreProcessor\n",
    "\n",
    "from src.HMM_utils import map_alphabet_to_numbers\n",
    "from src.HMM_utils import find_mapping, numbers_to_string, invert_mapping\n",
    "from src.HMM_utils import convert_numbers_to_letters\n",
    "\n",
    "# from src.HMM_functions import Baum_Welch\n",
    "# from src.HMM_functions import compute_f_log, Viterbi_log, reconstruct\n",
    "\n",
    "import numpy as np\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    \"\"\"\n",
    "    Given two strings a, b it returns a percentage of matching characters among the two\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(\n",
    "        np.array(list(a.replace(\" \", \"\"))) == np.array(list(b.replace(\" \", \"\")))\n",
    "    )\n",
    "\n",
    "\n",
    "def Homophonic_Cipher_Generator(extended_alphabet = list(\"abcdefghijklmnopqrstuvwxyz1234567890\")):\n",
    "    \"\"\"\n",
    "    Generates a permutation assigning to each letter one or potentially two of the symbols in the extended alphabet provided.\n",
    "    In this simple example it assigns letters as before + it assigns to some letters the numbers as well (can be generalized) \n",
    "    The output is kept as a dictionary.\n",
    "    \"\"\"\n",
    "    letters = extended_alphabet.copy()\n",
    "    letters = letters[0:26]\n",
    "    random.shuffle(letters)\n",
    "\n",
    "    letters_list = [letters[i:i+1] for i in range(0, len(letters), 1)]\n",
    "    for i in range(len(extended_alphabet) - 26):\n",
    "        index = random.randint(0, 25) # Select one of the existing letters\n",
    "        while(len(letters_list[index]) > 1):\n",
    "            index = random.randint(0, 25) # Select one of the existing letters\n",
    "        letters_list[index].append(i)\n",
    "\n",
    "    # print(letters_list)\n",
    "    d = {k:v for k,v in zip(extended_alphabet[0:26], letters_list)}\n",
    "    return(d)\n",
    "    \n",
    "def Encode_using_homophonic(text, cipher_dict):\n",
    "    \"\"\"\n",
    "    Encodes using the homophonic dictionary provided\n",
    "    We assume text has already been preprocessed to remove all puntctuation ...\n",
    "    \"\"\"\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            encoded_text.append(char)\n",
    "            continue\n",
    "            \n",
    "        if len(cipher_dict[char]) == 1:\n",
    "            encoded_char = cipher_dict[char]\n",
    "        else:\n",
    "            encoded_char = str(cipher_dict[char][random.randint(0,len(cipher_dict[char])-1)])  \n",
    "        \n",
    "        if isinstance(encoded_char, list):\n",
    "            encoded_char = str(encoded_char[0])\n",
    "        encoded_text.append(encoded_char)\n",
    "    \n",
    "    \n",
    "    return \"\".join(encoded_text)\n",
    "\n",
    "def forward_HMM(A, B, pi, observed):\n",
    "    \"\"\"\n",
    "    A: transition\n",
    "    B: emission\n",
    "    pi: initial\n",
    "    n_nodes: number of nodes in the chain\n",
    "    observed: list containing observed ones.\n",
    "    \"\"\"\n",
    "    n_nodes = len(observed)\n",
    "    n_states = A.shape[0]\n",
    "    alpha = np.zeros((n_nodes, n_states))\n",
    "    c = np.zeros(n_nodes)\n",
    "    alpha_hat = np.zeros((n_nodes, n_states))\n",
    "\n",
    "    for j in range(n_states):\n",
    "        alpha[0, j] = pi[j] * B[j, observed[0]]\n",
    "\n",
    "    c[0] = np.sum(alpha[0])\n",
    "    alpha_hat[0] = alpha[0] / np.sum(alpha[0])\n",
    "    # print(\"alpa[0]\", alpha[0])\n",
    "\n",
    "    for i in range(1, n_nodes):\n",
    "        for j in range(n_states):\n",
    "            for k in range(n_states):\n",
    "                alpha[i, j] += A[k, j] * B[j, observed[i]] * alpha_hat[i - 1, k]\n",
    "        c[i] = np.sum(alpha[i])\n",
    "        alpha_hat[i] = alpha[i] / c[i]\n",
    "    return alpha_hat, c\n",
    "\n",
    "\n",
    "def backward_HMM(A, B, observed, c):\n",
    "    \"\"\"\n",
    "    A: transition\n",
    "    B: emission\n",
    "    n_nodes: number of nodes in the chain\n",
    "    observed: list containing observed ones.\n",
    "    \"\"\"\n",
    "    n_nodes = len(observed)\n",
    "    n_states = A.shape[0]\n",
    "    beta = np.zeros((n_nodes - 1, n_states))\n",
    "    beta_hat = np.zeros((n_nodes - 1, n_states))\n",
    "\n",
    "    for j in range(n_states):\n",
    "        for k in range(n_states):\n",
    "            beta[-1, j] += A[j, k] * B[k, observed[n_nodes - 1]]\n",
    "\n",
    "    beta_hat[-1] = beta[-1] / c[-1]\n",
    "\n",
    "    for i in range(n_nodes - 3, -1, -1):\n",
    "        for j in range(n_states):\n",
    "            for k in range(n_states):\n",
    "                beta[i, j] += A[j, k] * B[k, observed[i + 1]] * beta_hat[i + 1, k]\n",
    "        beta_hat[i] = beta[i] / c[i + 1]\n",
    "\n",
    "    return beta_hat\n",
    "\n",
    "\n",
    "def compute_all_conditional(alpha, beta):\n",
    "    \"\"\"\n",
    "    alpha: list containing forward messages\n",
    "    beta: list containing backward messages\n",
    "    \"\"\"\n",
    "    n_nodes = alpha.shape[0]\n",
    "    n_states = alpha.shape[1]\n",
    "\n",
    "    gamma = np.zeros((n_nodes, n_states))\n",
    "\n",
    "    gamma[n_nodes - 1] = alpha[n_nodes - 1] / np.sum(alpha[n_nodes - 1])\n",
    "\n",
    "    for i in range(n_nodes - 1):\n",
    "        tmp = alpha[i] * beta[i]\n",
    "        gamma[i] = tmp / np.sum(tmp)\n",
    "\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def divide_row_by_sum(matrix):\n",
    "    row_sums = np.sum(matrix, axis=1)  # Calculate the sum of each row\n",
    "    divided_matrix = (\n",
    "        matrix / row_sums[:, np.newaxis]\n",
    "    )  # Divide each element by the corresponding row sum\n",
    "    return divided_matrix\n",
    "\n",
    "\n",
    "def update_B(gamma, observed):\n",
    "    # n_nodes = gamma.shape[0]\n",
    "    n_states = gamma.shape[1]\n",
    "\n",
    "    B = np.zeros((n_states, 37))\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(37):\n",
    "            for k in range(len(observed)):\n",
    "                if observed[k] == j:\n",
    "                    B[i, j] += gamma[k, i]\n",
    "\n",
    "    return divide_row_by_sum(B)\n",
    "\n",
    "def Baum_Welch(A, B_start, pi, observed, maxIter=100, tol = 1e-4):\n",
    "    B = np.copy(B_start)\n",
    "    changed = 0 # change is set to 1 whenever at least one coordinate increases by more than tol\n",
    "    for it in range(maxIter):\n",
    "        #print(\"computing alpha\")\n",
    "        alpha_hat, c = forward_HMM(A, B, pi, observed)\n",
    "        #print(\"alpha_hat\", alpha_hat.sum(axis = 1))\n",
    "        #print(\"computing beta\")\n",
    "        beta_hat = backward_HMM(A, B, observed, c)\n",
    "        #print(\"beta_hat\", beta_hat.sum(axis = 1))\n",
    "        #print(\"computing gamma\")\n",
    "        gamma = compute_all_conditional(alpha_hat, beta_hat)\n",
    "        #print(\"gamma\", gamma.sum(axis = 1))\n",
    "        B_old = B\n",
    "\n",
    "        #print(\"updating B\")\n",
    "        #print(B.sum(axis = 1))\n",
    "        B = update_B(gamma, observed)\n",
    "        #print(\"computing B\", B.sum(axis = 1))\n",
    "        # Check if conerged or still changing\n",
    "        change = np.abs(B - B_old)\n",
    "        max_change = np.max(change)\n",
    "\n",
    "        if(max_change < tol):\n",
    "            print(\"Not updating anymore after iteration\", it)\n",
    "            break\n",
    "\n",
    "\n",
    "        # following lines only for encryption\n",
    "        B[:, -1] = np.zeros(27)\n",
    "        B[-1, :] = np.zeros(37)\n",
    "        B[-1, -1] = 1\n",
    "    return B\n",
    "\n",
    "\n",
    "def compute_f_log(A, B, observed):\n",
    "    \"\"\"\n",
    "    It constructs the factors of the HMM which are needed to perform the forward pass of the message passing algorithm.\n",
    "    Input:\n",
    "        - A : the transition matrix\n",
    "        - B : the emission matrix\n",
    "        - observed: an array containing the observed values\n",
    "    Output:\n",
    "        - f0: the factor corresponding to the initial factor to first latent variable message\n",
    "        - f: an array containig the all the other factors (n_states - 1)\n",
    "    \"\"\"\n",
    "    pi = A[-1]\n",
    "    n_nodes = len(observed)\n",
    "    n_states = A.shape[0]\n",
    "    f = np.zeros((n_nodes - 1, n_states, n_states))\n",
    "\n",
    "    tmp = np.zeros((n_states, 1))\n",
    "    for k in range(n_states):\n",
    "        tmp[k] = np.log(pi[k]) + np.log(B[k, observed[0]])\n",
    "\n",
    "    f0 = tmp\n",
    "\n",
    "    for i in range(1, n_nodes):\n",
    "        tmp = np.zeros((n_states, n_states))\n",
    "\n",
    "        for j in range(n_states):  # over z1\n",
    "            for k in range(n_states):  # over z2\n",
    "                tmp[j, k] = np.log(A[j, k]) + np.log(B[k, observed[i]])\n",
    "\n",
    "        f[i - 1] = tmp\n",
    "\n",
    "    return f0, f\n",
    "\n",
    "\n",
    "def Viterbi_log(f0, f):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of the max plus algorithm (known as Viterbi algorithm for Hidden-Markov models).\n",
    "    Input: \n",
    "        - f0: the factor corresponding to the initial factor to first latent variable message\n",
    "        - f: an array containig the all the other factors (n_states - 1)\n",
    "    Output:\n",
    "        - pmax: the array containing the messages of the forward pass\n",
    "        - phi: the array storing the most probable preceding state stored during the forward pass\n",
    "    \"\"\"\n",
    "    n_nodes = f.shape[0] + 1\n",
    "    n_states = f.shape[1]\n",
    "\n",
    "    pmax = np.zeros((n_nodes, n_states))  # Need one for every node\n",
    "    phi = np.zeros(\n",
    "        (n_nodes - 1, n_states)\n",
    "    )  # Need one for every node other than the first one (no need to reconstruct it)\n",
    "\n",
    "    pmax[0] = f0.flatten()\n",
    "\n",
    "    for i in range(1, n_nodes):\n",
    "        tmp = ((f[i - 1]).T + pmax[i - 1]).T\n",
    "\n",
    "        pmax[i] = np.max(tmp, axis=0)  # by column\n",
    "\n",
    "        phi[i - 1] = np.argmax(\n",
    "            tmp, axis=0\n",
    "        )  # i-1 cause this contains the reconstruction about the (i-1)th element\n",
    "\n",
    "    return pmax, phi\n",
    "\n",
    "\n",
    "def reconstruct(pmax, phi):\n",
    "    \"\"\"\n",
    "    Given the output of a max-plus forward pass it returns the most probable hidden states.\n",
    "    Input:\n",
    "        - pmax: the array containing the messages of the forward pass\n",
    "        - phi: the array storing the most probable preceding state stored during the forward pass\n",
    "    Output:\n",
    "        - An array of int that coincides with the most probable latent states\n",
    "    \"\"\"\n",
    "    reconstruction = np.empty(len(phi) + 1)\n",
    "\n",
    "    curr = np.argmax(pmax[-1])\n",
    "    reconstruction[-1] = curr\n",
    "\n",
    "    for i in range(len(phi) - 1, -1, -1):\n",
    "        curr = int(phi[i, curr])\n",
    "        reconstruction[i] = curr\n",
    "\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text file paths to build our corpus (where we learn the transitions probs)\n",
    "file_paths = [\n",
    "    \"texts/moby_dick.txt\",\n",
    "    \"texts/shakespeare.txt\",\n",
    "    \"texts/james-joyce-a-portrait-of-the-artist-as-a-young-man.txt\",\n",
    "    \"texts/james-joyce-dubliners.txt\",\n",
    "    \"texts/james-joyce-ulysses.txt\",\n",
    "]\n",
    "\n",
    "texts = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        texts.append(file.read())\n",
    "\n",
    "corpus = \"\".join(texts)\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "\n",
    "preprocessor = TextPreProcessor()\n",
    "corpus = preprocessor.lower(text=corpus)\n",
    "corpus = preprocessor.remove_unknown_chars(\n",
    "    text=corpus, unknown_chars=preprocessor.unknown_chars(corpus)\n",
    ")\n",
    "corpus = preprocessor.remove_additional_spaces(text=corpus)\n",
    "\n",
    "# compute probabilities\n",
    "p = ProbabilityMatrix(corpus)\n",
    "p.compute_probability_matrix()\n",
    "p.compute_normalized_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_numbers_updated(text, mapping):\n",
    "    \"\"\"\n",
    "    We convert a->0, b->1, ..., z->25, 0->26, 1->27, 2->28, ..., 9->35, ' '-> 36\n",
    "    \"\"\"\n",
    "    converted_string = [mapping[char] for char in text]\n",
    "    return converted_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = TextPreProcessor()\n",
    "def prepare_subtexts(path_to_file, lengths):\n",
    "    \"\"\"\n",
    "    This function returns the subtext to use to assess accuracy in decryption\n",
    "\n",
    "    Input:\n",
    "        - path_to_file (str) giving the path to the text file\n",
    "        - lengths (list of int) giving the text lengths to consider\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_file, \"r\") as input_file:\n",
    "        full_text = input_file.read()\n",
    "\n",
    "    full_text = preprocess.lower(full_text)\n",
    "    unknown_chars = preprocess.unknown_chars(full_text)\n",
    "    full_text = preprocess.remove_unknown_chars(full_text, unknown_chars=unknown_chars)\n",
    "    full_text = preprocess.remove_additional_spaces(full_text)\n",
    "\n",
    "    subtexts = [\n",
    "        \" \".join(full_text.split()[: lengths[i]]) for i in range(len(lengths))\n",
    "    ]  # Contains the ones for the varying lengths\n",
    "\n",
    "    return subtexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_varying_text_length_homophonic(\n",
    "    subtexts,\n",
    "    probability_matrix,\n",
    "    n_iterations = 3,\n",
    "    maxIter_BM=100,\n",
    "    tol_BM=10e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function assesses the accuracy of the MCMC decryption + that of the HMM decryption process\n",
    "    Input:\n",
    "        - subtexts (list of str): a vector of text of different length\n",
    "        - cipher_generator (CipherGenerator instance)\n",
    "        - probability_matrix\n",
    "        - extract_top(int) : After cipher breaking how many of the top ones we want to retain\n",
    "        - n_iterations (int) : number of times we want to repeat the encryption-decryption for each text\n",
    "        - max_iterations (int) : number of iterations in the cipher breaking procedure\n",
    "        - nstart (int) : number of starting points in the cipher breaking procedure\n",
    "        - maxIter_BM (int) = number of maximum number of iterations that can be performed by the BM algorithm\n",
    "        - tol_BM (float) = after no updates larger than this we stop BM algorithm\n",
    "\n",
    "    For each subtext it encrypts it and decrypts it n_iteration times.\n",
    "        Everytime it finishes decrypting it compares the best extract_top (in terms of log likelihood) with the original string,\n",
    "        and computes the accuracy as the proportion of characters matching for the string which matches best.\n",
    "        Then for each subtext these are averaged over all n_iterations runs and are stored.\n",
    "    \"\"\"\n",
    "    mean_accuracy_HMM_viterbi = []\n",
    "\n",
    "    for subtext in subtexts:\n",
    "        total_iterations_HMM_viterbi = 0\n",
    "\n",
    "        # We run n_iterations ciphers and measure the averge accuracy on those.\n",
    "        for _ in range(n_iterations):\n",
    "            cipher = Homophonic_Cipher_Generator()\n",
    "            encoded_text = Encode_using_homophonic(subtext, cipher)\n",
    "\n",
    "            mapping = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9,\n",
    "           'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18,\n",
    "           't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25,\n",
    "           '0': 26, '1': 27, '2': 28, '3': 29, '4': 30, '5': 31, '6': 32, '7': 33, '8': 34,\n",
    "           '9': 35, ' ': 36}\n",
    "            \n",
    "            hidden_ = string_to_numbers_updated(encoded_text, mapping = mapping)\n",
    "            #print(hidden_)\n",
    "\n",
    "            observed_ = string_to_numbers_updated(subtext, mapping)\n",
    "            #print(observed_)\n",
    "\n",
    "\n",
    "            ######################################################### HMM #########################################################\n",
    "            B_start = np.zeros((27, 37)) + 1 / 36\n",
    "            B_start[:, -1] = np.zeros(27) # last column\n",
    "            B_start[-1, :] = np.zeros(37) # last row\n",
    "            B_start[-1, -1] = 1           # last entr\n",
    "            emission = Baum_Welch(\n",
    "                A=probability_matrix.normalized_matrix,\n",
    "                B_start=B_start,\n",
    "                pi=probability_matrix.normalized_matrix[-1, :],\n",
    "                observed=observed_,\n",
    "                maxIter=maxIter_BM,\n",
    "                tol=tol_BM,\n",
    "            )\n",
    "\n",
    "            # Now we use Viterbi to obtain the most likely one\n",
    "            f0, f = compute_f_log(\n",
    "                A=probability_matrix.normalized_matrix, B=emission, observed=observed_\n",
    "            )\n",
    "            pmax, phi = Viterbi_log(f0, f)\n",
    "            reconstruction = reconstruct(pmax, phi)\n",
    "            reconstruction = reconstruction.astype(int)\n",
    "            viterbi_reconstruction = convert_numbers_to_letters(reconstruction)\n",
    "            total_iterations_HMM_viterbi = total_iterations_HMM_viterbi + similar(\n",
    "                subtext, viterbi_reconstruction\n",
    "            )\n",
    "\n",
    "        mean_accuracy_HMM_viterbi.append(total_iterations_HMM_viterbi / n_iterations)\n",
    "\n",
    "    return mean_accuracy_HMM_viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not updating anymore after iteration 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tv/ysw0jsvs2t331mtk7hdh35kc0000gn/T/ipykernel_3384/2983748478.py:206: RuntimeWarning: divide by zero encountered in log\n",
      "  tmp[k] = np.log(pi[k]) + np.log(B[k, observed[0]])\n",
      "/var/folders/tv/ysw0jsvs2t331mtk7hdh35kc0000gn/T/ipykernel_3384/2983748478.py:215: RuntimeWarning: divide by zero encountered in log\n",
      "  tmp[j, k] = np.log(A[j, k]) + np.log(B[k, observed[i]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not updating anymore after iteration 56\n",
      "Not updating anymore after iteration 56\n",
      "Not updating anymore after iteration 69\n",
      "Not updating anymore after iteration 69\n",
      "Not updating anymore after iteration 69\n",
      "Not updating anymore after iteration 45\n",
      "Not updating anymore after iteration 45\n",
      "Not updating anymore after iteration 45\n",
      "Not updating anymore after iteration 29\n",
      "Not updating anymore after iteration 29\n",
      "Not updating anymore after iteration 29\n",
      "Not updating anymore after iteration 30\n",
      "Not updating anymore after iteration 30\n",
      "Not updating anymore after iteration 30\n",
      "Not updating anymore after iteration 34\n",
      "Not updating anymore after iteration 34\n",
      "Not updating anymore after iteration 34\n",
      "Not updating anymore after iteration 34\n",
      "Not updating anymore after iteration 34\n",
      "Not updating anymore after iteration 34\n"
     ]
    }
   ],
   "source": [
    "# Use it on the war message text\n",
    "lengths = [10, 25, 50, 100, 250, 500, 1000]\n",
    "subtexts = prepare_subtexts(\"texts/eisenhower_speech.txt\", lengths)\n",
    "\n",
    "mean_accuracies = speech_accuracy_HMM_viterbi = accuracy_varying_text_length_homophonic(subtexts, probability_matrix=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.425531914893617,\n",
       " 0.7043478260869566,\n",
       " 0.7914893617021277,\n",
       " 0.9473684210526315,\n",
       " 0.9617723718505647,\n",
       " 0.9706770159551531,\n",
       " 0.9709153122326774]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
