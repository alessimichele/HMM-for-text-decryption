{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observed_sequence, num_states, num_emissions, num_iterations):\n",
    "    # Initialize emission probabilities randomly\n",
    "    # emission_probs = np.random.rand(num_states, num_emissions)\n",
    "    emission_probs = np.full((num_states, num_emissions), 1 / 27)\n",
    "    emission_probs /= np.sum(emission_probs, axis=1, keepdims=True)\n",
    "\n",
    "    # Initialize the observed sequence and its length\n",
    "    observed_sequence = np.array(observed_sequence)\n",
    "    T = len(observed_sequence)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Step 1: Forward-Backward Algorithm\n",
    "\n",
    "        # Forward variables\n",
    "        alpha = np.zeros((T, num_states))\n",
    "\n",
    "        # Calculate alpha(1, i)\n",
    "        alpha[0] = emission_probs[:, observed_sequence[0]]\n",
    "\n",
    "        # Calculate alpha(t, i) for t > 1\n",
    "        for t in range(1, T):\n",
    "            for i in range(num_states):\n",
    "                alpha[t, i] = (\n",
    "                    np.sum(alpha[t - 1] * transition_probs[:, i])\n",
    "                    * emission_probs[i, observed_sequence[t]]\n",
    "                )\n",
    "\n",
    "        # Backward variables\n",
    "        beta = np.zeros((T, num_states))\n",
    "\n",
    "        # Set beta(T, i) = 1\n",
    "        beta[T - 1] = 1\n",
    "\n",
    "        # Calculate beta(t, i) for t < T\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for i in range(num_states):\n",
    "                beta[t, i] = np.sum(\n",
    "                    beta[t + 1]\n",
    "                    * transition_probs[i, :]\n",
    "                    * emission_probs[:, observed_sequence[t + 1]]\n",
    "                )\n",
    "\n",
    "        # Step 2: Estimation Step\n",
    "\n",
    "        # Compute gamma variables\n",
    "        gamma = alpha * beta / np.sum(alpha * beta, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute xi variables\n",
    "        xi = np.zeros((T - 1, num_states, num_states))\n",
    "        for t in range(T - 1):\n",
    "            for i in range(num_states):\n",
    "                for j in range(num_states):\n",
    "                    xi[t, i, j] = (\n",
    "                        alpha[t, i]\n",
    "                        * transition_probs[i, j]\n",
    "                        * emission_probs[j, observed_sequence[t + 1]]\n",
    "                        * beta[t + 1, j]\n",
    "                    ) / np.sum(\n",
    "                        alpha[t, :]\n",
    "                        * transition_probs[:, j]\n",
    "                        * emission_probs[j, observed_sequence[t + 1]]\n",
    "                        * beta[t + 1, j]\n",
    "                    )\n",
    "\n",
    "        # Step 3: Maximization Step\n",
    "\n",
    "        # Update emission probabilities\n",
    "        for i in range(num_states):\n",
    "            for k in range(num_emissions):\n",
    "                emission_probs[i, k] = np.sum(\n",
    "                    gamma[:, i] * (observed_sequence == k)\n",
    "                ) / np.sum(gamma[:, i])\n",
    "\n",
    "    return emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jt yinwctp je riiwr ed fi mnieep wbqx cbedwcejq mnieep wbqx ckk exi ejwi jt znctqi ctg rmcjt je ckk gimitgr mnirbwcfkp dt rdqjck rbfekiejir exce pdb xcvi ed fi znitqx dn rmctjrx ed btginrectg jt jeckp sxp sdbkg pdb ivit fdexin sxit ctg xds wbqx ed ejm jr c hbirejdt exce xcr fiit viajty vjrjednr ed ibndmi zdn cr kdty cr midmki xcvi fiit encvikkjty cndbtg exi qdtejtite dberjgi exijn dst qdbtenp je riiwr ivit ibndmictr gdt e ltds exi ctrsin cqqdngjty ed tis mdkkjty fp pdbydv jt rja ib qdbtenjir fnjecjt ctg exi br sxini cr wdre vjrjednr ltds fbe wcp fi nikbqecte ed cqltdskigyi yncebjejir wcp wcli bm wdni exct xckz pdbn scjeminrdt r jtqdwi ibndmictr cni giimkp gjvjgig dt ejmmjty jt nirecbncter zdn iacwmki dz nirmdtgiter jt yinwctp edkg exi mdkkrein exip epmjqckkp ejmmig ckwdre exi rcwi cr exi br jt exi bl sxini ct dmejdtck rinvjqi qxcnyi dz cfdbe jr brbckkp jtqkbgig rcjg exip kize c yncebjep exi zjybni jt rmcjt sxini rinvjqi jr dzeit jtqkbgig jt nirecbncte fjkkr fbe gjtinr qct kicvi dmejdtck ejmr scr sxjki jt znctqi sxini ivinp mnjqi dt c nirecbncte witb cknicgp jtqkbgir zdn rinvjqi dz midmki rcjg exip yitinckkp ejmmig dt edm ivit jt rsigit sxini ejmr cni yitinckkp tde iamiqeig exi zjybni scr fbe dtkp dz jeckjctr rcjg exip sdbkg epmjqckkp kicvi c yncebjep czein c wick dbe sjex c ncexin ynicein mndmdnejdt cgwjeejty exip tivin kize c qite c recnekjty dz nirmdtgiter jt exi br xdsivin ctg dz yinwctr fp zcn exi wdre jt ibndmi qdtzirrig exip sdbkg ejm rdwiejwir dn dzeit ivit jz exi rinvjqi scr einnjfki jtgjqcejty exce zdn rdwi ejmmjty jr tde cfdbe hbckjep dz rinvjqi ce ckk exi zjtgjtyr dz exi rbnvip sjkk qdwi cr c rbnmnjri jt yinwctp c qdbtenp exce gdir tde yitinckkp exjtl dz jerikz cr c tcejdt dz xcmmp gjrenjfbednr dz enjtlyikg exi sdng jt c rjwjkcn vijt ed mdbnfdjni jt znitqx wictr gnjtljty wdtip tdn jr yinwctp c qdbtenp sxdri rinvjqi reczz cni mcnejqbkcnkp qbredwin znjitgkp jt finkjt c ykcrr dz fiin dn c mkcei dz zddg jr dzeit rinvig jt c xbzz ncexin exct sjex c rwjki tde zdn tdexjty jr rinvjqis rei giberqxkctg yinwctp jr c rinvjqi girine c exjty pie exi gcec nizkiqer c qkicn enitg yinwctr eitg ed xctg dvin c zis iaenc qdjtr jnnirmiqejvi dz xds exip xcvi fiit eniceig exip cni exi tcejdt jt ibndmi kicre kjlikp ed tde ejm ivit zdn mddn rinvjqi ctg exi wdre sjkkjty ed mcp iaenc zdn cvincyi rinvjqi c qdtejtbjty zdtgtirr zdn qcrx wcp fi c zcqedn sjex wctp ecaj gnjvinr ctg fcnr rejkk nizbrjty mcpwite fp qcng c yndsjty cscnitirr dz kcfdbn rxdnecyir jt exi qceinjty riqedn qdbkg ckrd iamkcjt c qxctyi jt ceejebgi sjex wctp fcnr ctg qczir ivit jt fjy qjejir nigbqjty exijn dmitjty xdbnr gbi ed c mdre mctgiwjq kcql dz reczz rdwi fcnliiminr rcp exijn qbredwinr xcvi niqitekp fiqdwi wdni yitindbr ejmminr fbe jz yinwct ejmmjty xcfjer riiw jtyncjtig exip cni\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from src.CipherUtils import CipherGenerator\n",
    "from src.CipherUtils import TextDecoder, TextEncoder\n",
    "\n",
    "\n",
    "hidden_sequence = \"people of western europe a landing was made this morning on the coast of france by troops kangaroo jokes quasi vile xilophone zenit \"\n",
    "hidden_sequence = \"in germany it seems to be pretty much automatic pretty much all the time in france and spain it all depends presumably on social subtleties that you have to be french or spanish to understand in italy why would you even bother when and how much to tip is a question that has been vexing visitors to europe for as long as people have been travelling around the continent outside their own country it seems even europeans don t know the answer according to new polling by yougov in six eu countries britain and the us where as most visitors know but may be reluctant to acknowledge gratuities may make up more than half your waitperson s income europeans are deeply divided on tipping in restaurants for example of respondents in germany told the pollster they typically tipped almost the same as the us in the uk where an optional service charge of about is usually included said they left a gratuity the figure in spain where service is often included in restaurant bills but diners can leave optional tips was while in france where every price on a restaurant menu already includes for service of people said they generally tipped on top even in sweden where tips are generally not expected the figure was but only of italians said they would typically leave a gratuity after a meal out with a rather greater proportion admitting they never left a cent a startling of respondents in the us however and of germans by far the most in europe confessed they would tip sometimes or often even if the service was terrible indicating that for some tipping is not about quality of service at all the findings of the survey will come as a surprise in germany a country that does not generally think of itself as a nation of happy distributors of trinkgeld the word in a similar vein to pourboire in french means drinking money nor is germany a country whose service staff are particularly customer friendly in berlin a glass of beer or a plate of food is often served in a huff rather than with a smile not for nothing is servicew ste deutschland germany is a service desert a thing yet the data reflects a clear trend germans tend to hand over a few extra coins irrespective of how they have been treated they are the nation in europe least likely to not tip even for poor service and the most willing to pay extra for average service a continuing fondness for cash may be a factor with many taxi drivers and bars still refusing payment by card a growing awareness of labour shortages in the catering sector could also explain a change in attitude with many bars and cafes even in big cities reducing their opening hours due to a post pandemic lack of staff some barkeepers say their customers have recently become more generous tippers but if german tipping habits seem ingrained they are\"\n",
    "\n",
    "\n",
    "cipher_generator = CipherGenerator()\n",
    "cipher = cipher_generator.generate_cipher()\n",
    "encoder = TextEncoder()\n",
    "observed_sequence = encoder.encode_text(hidden_sequence, cipher=cipher)\n",
    "\n",
    "\n",
    "print(observed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prove.src.Probability import ProbabilityMatrix\n",
    "\n",
    "with open(\"texts/moby_dick.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "\n",
    "\n",
    "pm = ProbabilityMatrix(text=text, alphabet=alphabet)\n",
    "unknown_chars = pm.unknown_chars()\n",
    "pm.preprocess_text(unknown_chars=unknown_chars)\n",
    "\n",
    "# compute probabilities\n",
    "pm.compute_matrix_spaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = pm.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_alphabet_to_numbers():\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "    mapping = {char: i for i, char in enumerate(alphabet)}\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_string_to_numbers(text, mapping):\n",
    "    numbers = [mapping[char] for char in text]\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_ = translate_string_to_numbers(\n",
    "    hidden_sequence, mapping=map_alphabet_to_numbers()\n",
    ")\n",
    "observed_ = translate_string_to_numbers(\n",
    "    observed_sequence, mapping=map_alphabet_to_numbers()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 188\n",
      "1 33\n",
      "2 65\n",
      "3 69\n",
      "4 295\n",
      "5 57\n",
      "6 51\n",
      "7 82\n",
      "8 173\n",
      "9 0\n",
      "10 11\n",
      "11 88\n",
      "12 49\n",
      "13 178\n",
      "14 152\n",
      "15 65\n",
      "16 2\n",
      "17 162\n",
      "18 144\n",
      "19 203\n",
      "20 73\n",
      "21 38\n",
      "22 37\n",
      "23 8\n",
      "24 60\n",
      "25 0\n",
      "26 499\n"
     ]
    }
   ],
   "source": [
    "for i in range(27):\n",
    "    print(i, hidden_.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/r059lv3n7nq9x6gh78v25xk80000gn/T/ipykernel_95461/2565917047.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  gamma = alpha * beta / np.sum(alpha * beta, axis=1, keepdims=True)\n",
      "/var/folders/m2/r059lv3n7nq9x6gh78v25xk80000gn/T/ipykernel_95461/2565917047.py:56: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  xi[t, i, j] = (\n"
     ]
    }
   ],
   "source": [
    "emission = baum_welch(\n",
    "    observed_sequence=observed_,\n",
    "    num_emissions=27,\n",
    "    num_states=27,\n",
    "    num_iterations=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(emission.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the fixed transition probabilities\n",
    "transition_probs = np.array([[0.7, 0.3], [0.2, 0.8]])\n",
    "\n",
    "# Define the observed sequence\n",
    "observed_sequence = [0, 1, 0, 1, 0, 1]  # Example sequence of emissions\n",
    "\n",
    "# Set the number of states and emissions\n",
    "num_states = transition_probs.shape[0]\n",
    "num_emissions = (\n",
    "    2  # In this example, we assume there are only two possible emissions: 0 and 1\n",
    ")\n",
    "\n",
    "# Set the number of iterations for Baum-Welch algorithm\n",
    "num_iterations = 100\n",
    "\n",
    "# Estimate the emission probabilities using Baum-Welch algorithm\n",
    "emission_probs = baum_welch(\n",
    "    observed_sequence, num_states, num_emissions, num_iterations\n",
    ")\n",
    "\n",
    "print(\"Estimated emission probabilities:\")\n",
    "print(emission_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
